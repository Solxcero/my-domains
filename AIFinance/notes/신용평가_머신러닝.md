## 피처 엔지니어링
`IV` 와 `WoE`는 변수 선택과 변환에 있어 핵심적인 지표롤 활용됨. 또한 이산화 또는 구간화는 연속 변수를 의미 있는 구간으로 나누어 모델의 예측력을 향상시키는 방법. 

### `WoE : Weight of Evidence`  
는 주로 범주형 변수의 효과를 측정하는 데 사용. 변수의 각 범주가 타깃 변수에 미치는 영향의 강도와 방향을 나타냄.
    1. 연속형 변수의 경우, 데이터를 여러 구간(보통 10개 정도)의 버킷(bin)으로 분할
    2. 각 버킷에서 타깃 사건(예: 연체)과 비타깃 사건의 수를 집계
    3. 각 버킷에서 타깃 사건의 비율과 비타깃 사건의 비율을 계산
    4. 각 버킷의 타깃 사건 비율을 비타깃 사건 비율로 나누고, 자연로그를 취해 WoE 계산

### `IV : Informatino Value` 
는 이러한 WoE 값들을 사용해 전체 변수의 예측력을 수치화하는 지표로, 변수가 모델을 통헤 예측하는 데 얼마나 중요한지를 평가. 
    - 0.02 이하 : 예측에 유용하지 않음
    - 0.02 ~ 0.1 : 약한 예측력
    - 0.1 ~ 0.3 : 보통의 예측력
    - 0.3 ~ 0.5 : 강한 예측력
    - 0.5 이상 (너무 강력해서) 의심스러운 예측력

- 예제 : `나이` 면수로 WoE와 IV 계산 -> 나이를 10년 단위로 구분 

```python 
# 연령대 20~29의 연체 고객 수와 비연체 구객 수
age_20_29_default = 10
age_20_29_non_default = 90

# 전체 대출 고객 중 연체 고객 수와 비연체 고객 수
tatal_default = 100
total_non_default = 900

# 연령대 20~29의 연체 비율과 비연체 비율 계산
default_rate_20_29 = age_20_29_default / total_default
non_default_rate_20_29 = age_20_29_non_default / total_non_default

# WoE 계산
woe_20_29 = np.log(default_rate_20_29 / non_default_rate_20_29)
woe_20_29_value = woe_20_29.item() if woe_20_29.size > 0 else "Undefined"
```

위의 과정을 전 나이대에서 수행. 

IV = (각 연령대의 연체 사건 비율 - 비연체 사건 비율) * WoE

### fine classing 
변수 구간 나누는 기법1
- 변수를 매우 세밀한 구간으로 나눔. (나이를 5년 단위로 나누기 등) -> 더 정밀한 예측 가능 but 과적합 위험있으므로 충분한 데이터 확보된 경우에 주로 사용

### coarse classing
변수 구간 나누는 기법2
- 보다 큰 구간으로 변수 나눔 (나이를 10년 단위로 나누기 등) -> 데이터 부족하거나 과적합 방지하고자 할 때
-----------------------------

신용 평가에서 주요 과제는 충분한 정보를 유지하면서도 모델의 복잡성을 적절히 관리하는 것. 

### 변수 선택
1. 필터 방법 
- 데이터의 특성과 목표 변수와의 관련성을 수치화하여 변수를 선택하는 기법. 
- 피어슨 상관계수, 카이제곱 검정, ANOVA, 피셔 점수, IV 등을 포함한 다양한 통계적 방법 사용
- 계산이 빠르고 간단. 하지만 변수 간의 상호작용은 고려하지 않음 
2. 래퍼 방법 
- 모델의 예측 성능을 기준으로 변수의 중요도를 평가
- RFE(recursive feature elimination), 후진 소거법, 전진 선택법 등
- 변수 선택 과정이 모델 훈련과 밀접하게 연결되어 있어, 변수 간의 상호작용도 고려할 수 있음. 하지만 계산 비용이 높음.
- 예측 정확도 최적화하는 동시에 필요한 변수 수를 최소화하려는 목적으로 사용
3. 임베디드/모델 기반 방법 
- 모델 자체에서 제공하는 변수의 중요도를 기반으로 변수 선택
- 피쳐중요도, 정규화 기법
- 모델 성능과 변수 간 상호작용 모두 고려하며 계산 비용 적절. 


## 스코어링
신용 평가 모델에서 확률을 사용하여 신용 점수를 계산하는 방법은 모델이 제공하는 확률값을 기반으로 고객의 신용 위험을 수치화함.   
이 과정에서 `Base Score (모든 고객에게 기본적으로 할당되는 점수)`와 `PDO (point to double the odds, 확률의 변화가 신용점수에 얼마나 영향을 미치는지)`가 중요한 역할을 함.  

**점수 계산 순서**
1. 모델 확률값의 취득 : 신용 평가 모델(ex.XGBoost)을 통해 고객의 연체 가능성에 대한 확률값을 산출. 이 확률값은 고객이 특정 시간 내에 연체할 가능성을 나타냄
2. Odds 계산 : 확률값(P)을 사용하여 Odds 계산. Odds는 연체 가능성에 대한 확률값에 대한 비율로, `P/(1-P)` 공식으로 계산. 이는 고객이 연체할 가능성 대비 연체하지 않을 가능성의 비율
3. PDO와 Base Score 적용
    - Factor 계산 : `Factor = PDO/ln(2)`. 확률의 변화가 신용 점수에 미치는 영향력 조정
    - Offset 계산 : `Offset = Base Score - (Factor * ln(Target Odds))` . Target Odds는 모델 개발 시 선정한 기준 Odds
4. 최종 신용 점수 계산 : `score = Offset + (Factor * ln(Odds))`. 계산한 신용 점수는 일반적으로 특정 범위 내에서 조정할 수 있으며, 이는 금융기관의 요구 사항에 따라 달라질 수 있음.


## 모델 해석력
### 로컬 해석력
모델이 특정 개별 예측을 내릴 때 어떤 변수가 주요하게 작용했는지 설명.   
이는 모델의 결정 과정을 특정 사례에서 투명하게 만들어 해당 결정에 대한 신뢰성을 높임. 

1. LIME (local interpretable model-agnostic explanation)
- 개별 데이터 포인트 주변에서 간단한 모델(선형모델 등)을 학습하여 해당 포인트에서 복잡한 모델의 예측을 근사하고 설명
2. SHAP (shapley additive explanation)
- 각 특성이 개별 예측에 얼마나 기여했는지 설명하는 데 사용. 
- 각 데이터 포인트에 대한 예측을 분해하여 특성 수준에서의 기여도 계싼
3. 반사실적 설명 (counterfactual explanation)
- 데이터 포인트를 약간 변경하여 모델 예측이 어떻게 바뀌는지 관찰. 해당 데이터 포인트에서 모델 예측이 어떤 특성에 크게 영향을 받았는지 파악 가능

### 글로벌 해석력
모델 전체의 작동 방식을 이해하는 데 초점. 

1. 순열 중요도 (permutation importance)
- 각 특성을 하나씩 무작위로 섞어서 모델 성능에 얼마나 영향을 미치는지 측정
- 특성을 제거했을 때 모델 성능이 크게 저하되면 그 특정은 중요한 요인으로 간주
2. PDP (partial dependence plot)
- 특정 특성값이 변할 때 모델 예측에 어떤 영향을 미치는지 보여주는 그래프.
3. SHAP
- 각 특성이 예측에 얼마나 기여하는지 평가. 
4. 상관관계 분석
- 특성이 예측에 어떻에 영향을 미치는지 파악. 선형모델에서 특히 유용
5. KAN (kolmogrov-Arnold network) 알고리즘
- 복잡한 다변수 함수를 단일 변수 함수와 단순한 연산으로 분해하는 신경망. (최근 주목)
- 모델 예측 과정을 전반적으로 해석 + 변수 기여도 파악 + 함수 근사 문제를 효과적으로 해결하고 모델의 글로벌 해석 가능성 높임.

## 모델 배포
1. 웹 서비스 배포 : 플라스크, 장고와 같은 웹 프레임워크를 사용하여 모델을 REST API로 만들고 웹 서비스 배포. 이 방법을 통해 웹 브라우저나 다른 애플리케이션에서 HTTP요청을 통해 모델 이용.

2. 클라우드 서비스 : amazon sagemaker, google cloud AI platform, Azure Machine learning 같은 클라우드 플랫폼을 이용해 모델 배포. 모델 학습 + 배포 + 모니터링 등 과정 관리 기능 제공

3. 머신러닝 모델 서버 이용 : TensorFlow Serving, NVIDIA Triton Interface Serer, Sheldon 과 같은 머신러닝 모델 서버를 이용할 수 있음. 모델의 배포와 관리 효율적으로 수행하는 기능 제공

+ 온라인 예측(실시간 처리), 배치 예측(주기적으로 대용량 처리)

## 모니터링
데이터 분포의 변화, 즉 데이터 분포 시트 주목.  
모델이 처음 훈련된 데이터 분포를 원본 분포라고 한다면, 모델이 실제 운영 환경에서 만나게 되는 데이터의 분포는 대상 분포. 아래는 데이터 분포 시프트 종류

1. 공변량 시프트 : 입력 데이터의 분포가 변하는 경우. 예시_ 경제 상황의 변화로 소비자 지출 패턴 변화
2. 레이블 시프트 : 출력 레이블의 분호가 변하는 경우. 예시_ 신용 위험이 증가하거나 감소하기는 시기에 발생 가능
3. 개념 드리프트 : 입력과 출력 사이의 관계가 시간에 따라 변하는 경우. 예시_ 신용 평가 기준 변경

이러한 변화를 감지하고 대응하기 위해 다양한 도구와 라이브러리 개발됨. 
- `Alibi Detect` 는 다양한 드리프트 감지 알고리즘을 구현한 오픈소스 패키지. 데이터 분포 시프트를 자동으로 감지하는 데 유용
- `Evidently` 와 `TFDV(tensorflow data validation)`와 같은 도구는 모델의 입력 데이터와 예측 데이트를 분석하여 시프트 감지
- `TOAD` 와 `OptBinning`을 활용해 PSI지표로 모델의 안정성 평가.

